{"summary":"Nuclear medicine image processing\nNumerical simulation and analysis\nComputational bottleneck hotspot analysis and execution efficiency optimization\nCluster construction and maintenance\nDeep learning","lastName":"Lin","objectUrn":"urn:li:member:706747763","geoRegion":"Taoyuan City, Taoyuan City, Taiwan","fullName":"Wen-Bin Lin","firstName":"Wen-Bin","currentPositions":[{"companyName":"National Atomic Research Institute","title":"Software Engineer","tenureAtCompany":{"numMonths":9},"startedOn":{"month":9,"year":2023}}],"entityUrn":"urn:li:fs_salesProfile:(ACwAACogHXMBJouE-MsI4FuxPKBxiG40paEegPo,NAME_SEARCH,vbiI)","profilePictureDisplayImage":{"artifacts":[{"width":100,"fileIdentifyingUrlPathSegment":"100_100\/0\/1701053626996?e=1723075200&v=beta&t=dywzcpqb_G2KINN_Bhr1FFB2nNnBt8fA-Dae4hrRYl0","height":100},{"width":200,"fileIdentifyingUrlPathSegment":"200_200\/0\/1701053626996?e=1723075200&v=beta&t=2oacYGSZKpcYv1cmOfunk2ba2B9UrvyctZ2Kc29w-Eg","height":200},{"width":389,"fileIdentifyingUrlPathSegment":"400_400\/0\/1701053626996?e=1723075200&v=beta&t=EqHCyL5qAticqsMOuSOQlSje69C69Oo7TnFARC3_cG8","height":389},{"width":389,"fileIdentifyingUrlPathSegment":"800_800\/0\/1701053626996?e=1723075200&v=beta&t=-X5xxf_SWM36TEbQiVD61d69h6r70lDImGeJ8boWQnY","height":389}],"rootUrl":"https:\/\/media.licdn.com\/dms\/image\/D5603AQEZoS_eKN4gWQ\/profile-displayphoto-shrink_"},"projects":[{"members":[{"entityUrn":"urn:li:fs_salesProfile:(ACwAACogHXMBJouE-MsI4FuxPKBxiG40paEegPo,NAME_SEARCH,vbiI)"}],"description":"\u4e3b\u4f5c\u7814\u767c\u9583\u720d\u6676\u9ad4\u6027\u80fd\u91cf\u6e2c\u7cfb\u7d71\uff0c\u5df2\u5b8c\u6210\u6280\u8f49\uff0c\u4fc3\u6210\u570b\u5167\u8a95\u751f\u7b2c\u4e00\u5bb6\uff0c\u4e5f\u662f\u5168\u7403\u7b2c5\u5bb6\uff0c\u5177\u751f\u9577\u9583\u720d\u6676\u9ad4\u80fd\u529b\u516c\u53f8\u3002\u9583\u720d\u6676\u9ad4\u662fPET\/CT\u548cPET\/MRI\u8a2d\u5099\u5f71\u50cf\u63a2\u982d\u6700\u91cd\u8981\u95dc\u9375\u96f6\u7d44\u4ef6\u4e4b\u4e00\uff0c\u63d0\u4f9b\u5177\u53ef\u4fe1\u80fd\u529b\u4e4b\u91cf\u6e2c\u5831\u544a\u70ba\u63a8\u92b7\u8ca9\u552e\u95dc\u9375\u56e0\u7d20\u4e4b\u4e00\u3002","title":"\u7814\u767c\u9583\u720d\u6676\u9ad4\u6027\u80fd\u91cf\u6e2c\u7cfb\u7d71"},{"members":[{"entityUrn":"urn:li:fs_salesProfile:(ACwAACogHXMBJouE-MsI4FuxPKBxiG40paEegPo,NAME_SEARCH,vbiI)"}],"description":"\u53c3\u8207\u6539\u826f\u5f0f\u7259\u79d1\u65b7\u5c64\u6383\u63cf\u6280\u8853\uff0c\u8ca0\u8cac\u7814\u767c\u65b0\u5f0f\u6383\u63cf\u65b9\u5f0f\u4e4b\u5f71\u50cf\u91cd\u5efa\u3001\u57f7\u884c\u6548\u7387\u6539\u5584\u7b49\uff0c\u6b64\u8a08\u756b\u70ba\u5ee0\u5546\u63d0\u51fa\u9700\u6c42\uff0c\u7531\u539f\u51481\u5c0f\u6642\u8a08\u7b97\u6642\u9593\uff0c\u6539\u5584\u70ba2\u5206\u9418\u5167\uff0c\u9054\u5546\u696d\u6c34\u5e73\u3002","title":"\u6539\u826f\u5f0f\u7259\u79d1\u65b7\u5c64\u6383\u63cf\u6280\u8853"},{"members":[{"entityUrn":"urn:li:fs_salesProfile:(ACwAACogHXMBJouE-MsI4FuxPKBxiG40paEegPo,NAME_SEARCH,vbiI)"}],"description":"\u53c3\u8207Taiwan TomoDR\u6a5f\u53f0\u7814\u767c\uff0c\u9020\u5f71\u6d41\u7a0b\u898f\u5283\u8207\u5be6\u4f5c\u3001\u5716\u5f62\u5316\u64cd\u4f5c\u754c\u9762\u3001\u901a\u8a0a\u7de8\u78bc\u5354\u8b70\u898f\u5283\u8207\u958b\u767c\u3001\u914d\u5408\u91ab\u9662\u683c\u5f0f\u8f49\u6a94\uff0c\u5df2\u901a\u904e\u4eba\u9ad4\u8a66\u9a57\u59d4\u54e1\u6703(IRB)\u5be9\u67e5\uff0c\u9032\u5165\u81e8\u5e8a\u8a66\u9a57\u968e\u6bb5\u3002","title":"Taiwan TomoDR\u6a5f\u53f0\u7814\u767c"},{"members":[{"entityUrn":"urn:li:fs_salesProfile:(ACwAACogHXMBJouE-MsI4FuxPKBxiG40paEegPo,NAME_SEARCH,vbiI)"}],"description":"\u53c3\u8207INER BreastPET\u6a5f\u53f0\u7814\u767c\uff0c\u8ca0\u8cac\u5c07\u985e\u6bd4\u8a0a\u865f\u8f49\u70ba\u6578\u4f4d\u8a0a\u865f\u8f49\u865f\u8207\u7cfb\u7d71\u5167\u6a94\u6848\u683c\u5f0f\u8f49\u63db\uff0c\u81e8\u5e8a\u8a66\u9a57\u7b49\u8ca9\u552e\u524d\u6cd5\u898f\u8981\u6c42\u4e8b\u9805\u5df2\u9054\u6210\uff0c\u5df2\u5b8c\u6210\u6280\u8f49\u3002\u70ba\u570b\u5167\u9996\u90e8\u81ea\u88fd\u4e73\u623f\u5c08\u7528\u9ad8\u968e\u91ab\u7642\u5668\u6750\uff0c\u514d\u5207\u7247\u5373\u53ef\u5075\u6e2c\u816b\u7624\uff0c\u4e26\u7d93\u81e8\u5e8a\u8a66\u9a57\u8b49\u660e\uff0c\u6bd4\u5e02\u552e\u50b3\u7d71\u4e73\u623f\u651d\u5f71\u5100\u5177\u6709\u66f4\u4f73\u7684\u816b\u7624\u5075\u6e2c\u80fd\u529b\u3002","title":"INER BreastPET\u6a5f\u53f0\u7814\u767c"}],"contactInfo":{},"industry":"Medical Equipment Manufacturing","educations":[{"endedOn":{"year":2008},"degree":" Master's degree","eduId":570010461,"schoolUrn":"urn:li:fs_salesSchool:143144","school":"urn:li:fs_salesSchool:143144","fieldsOfStudy":["Computer Science and Information Engineering"],"schoolName":"Fu Jen Catholic University","startedOn":{"year":2006}},{"endedOn":{"year":2006},"degree":"Bachelor degree","eduId":570012027,"schoolUrn":"urn:li:fs_salesSchool:143144","school":"urn:li:fs_salesSchool:143144","fieldsOfStudy":[" Department of Mathematics"],"schoolName":"Fu Jen Catholic University","startedOn":{"year":2001}}],"skills":[{"numOfEndorsement":0,"name":"\u8edf\u9ad4\u8a2d\u8a08"},{"numOfEndorsement":0,"name":"Python"},{"numOfEndorsement":0,"name":"\u8cc7\u6599\u5206\u6790"},{"numOfEndorsement":0,"name":"\u6f14\u7b97\u6cd5\u8a2d\u8a08"},{"numOfEndorsement":0,"name":"Databases"},{"numOfEndorsement":1,"name":"Artificial Intelligence (AI)"},{"numOfEndorsement":0,"name":"Nuclear Medicine"},{"numOfEndorsement":1,"name":"Machine Learning"},{"numOfEndorsement":0,"name":"Matlab"},{"numOfEndorsement":0,"name":"Microsoft SQL Server"},{"numOfEndorsement":0,"name":"Transact-SQL (T-SQL)"},{"numOfEndorsement":0,"name":"MySQL"},{"numOfEndorsement":0,"name":"Software Development Life Cycle (SDLC)"},{"numOfEndorsement":0,"name":"IEC 62304"},{"numOfEndorsement":1,"name":"Data Science"},{"numOfEndorsement":0,"name":"C (Programming Language)"},{"numOfEndorsement":0,"name":"Python (Programming Language)"},{"numOfEndorsement":0,"name":"MariaDB"},{"numOfEndorsement":0,"name":"Algorithm Development"},{"numOfEndorsement":0,"name":"Research"},{"numOfEndorsement":0,"name":"Group Work"},{"numOfEndorsement":0,"name":"C++"},{"numOfEndorsement":0,"name":"C"},{"numOfEndorsement":0,"name":"Teamwork"},{"numOfEndorsement":0,"name":"Deep Learning"},{"numOfEndorsement":0,"name":"Convolutional Neural Networks (CNN)"}],"numOfConnections":37,"patents":[],"headline":"Software Engineer","courses":[{"name":"2019 Google Cloud OnBoard"},{"name":"2019 AI\u5546\u6230\u71df"}],"certifications":[{"companyUrn":"urn:li:fs_salesCompany:3526187","url":"https:\/\/www.coursera.org\/account\/accomplishments\/specialization\/certificate\/QV36CUSPVU33","authority":"Coursera Course Certificates","name":"Machine Learning with TensorFlow on Google Cloud Platform","company":"urn:li:fs_salesCompany:3526187","licenseNumber":"QV36CUSPVU33","startedOn":{"month":9,"year":2019}},{"companyUrn":"urn:li:fs_salesCompany:2746406","url":"https:\/\/courses.edx.org\/certificates\/f9cc4914d9564b5782e7783874994fdc","authority":"edX","name":"edX Verified Certificate for Microsoft Professional Capstone : Data Science","company":"urn:li:fs_salesCompany:2746406","licenseNumber":"f9cc4914d9564b5782e7783874994fdc","startedOn":{"month":1,"year":2019}},{"companyUrn":"urn:li:fs_salesCompany:2746406","url":"https:\/\/courses.edx.org\/certificates\/caf8cc1e87274e89b3b9362d0b1136b6","authority":"edX","name":"edX Verified Certificate for Microsoft Professional Capstone : Artificial Intelligence","company":"urn:li:fs_salesCompany:2746406","licenseNumber":"caf8cc1e87274e89b3b9362d0b1136b6","startedOn":{"month":1,"year":2019}}],"memberBadges":{"premium":false,"openLink":false,"jobSeeker":false},"flagshipProfileUrl":"https:\/\/www.linkedin.com\/in\/tedslin","organizations":[],"location":"Taoyuan City, Taoyuan City, Taiwan","publications":[{"name":"Using CUDA and Multi-GPU to Accelerate Geometry-Factor Calculation for PEM Reconstruction","publishedOn":{"year":2009},"description":"Iterative image reconstruction is used commonly to obtain better quality tomographical imaging, such as positron emission tomography, x-ray computed tomography, etc. However, it is time consuming to process iterative reconstruction especially using complicated geometrical model for high resolution image. CUDA, the Common Unified Device Architecture, is a novel programming framework for massively parallel processing by GPU computing. To reduce calculation time, we apply this technology to solid-angle geometrical model, which is the main time consuming step in the three-dimensional reconstruction of positron emission mammography (PEM). Moreover, Multi-GPU technique is also applied for comparison.","publisher":"Conference on Computational Physics","authors":[{"entityUrn":"urn:li:fs_salesProfile:(ACwAACogHXMBJouE-MsI4FuxPKBxiG40paEegPo,NAME_SEARCH,vbiI)"}]},{"name":"Near-infrared optical imaging of 3D-breast model with diffuse photon-pair density waves","publishedOn":{"year":2010},"description":"Aim Near-infrared Diffuse Optical Tomography has unique capabilities for imaging functional parameters such as hemoglobin concentration and oxygen saturation of the tissue. It is a promising technique applied to breast and brain imaging. And the instrumentation is noninvasive, non-ionizing, inexpensive, and portable. In this study the feasibility of diffuse photon-pair density waves (DPPDW) and MRI-breast model were simulated using ASAP BIO Toolkit (Breault Research Organization, Inc.).Results The initial results shown that the global SNR was found to be greater when the breast thickness was 42mm (SNR=5 for 15mm tumor at \u00b5a=0.3 mm\u00af\u00b9) than 54mm (SNR=3.66 for 15mm tumor at \u00b5a=0.3 mm\u00af\u00b9). The global SNRs were 6.75, 5, 3.66 with 42mm breast thickness for 20mm, 15mm and 10mm tumor size respectively. Conclusion We set up a flexible diffuse optical simulation platform for breast imaging researches, specifically using DPPDW concept and 3D-breast model. In the current simulation model the coherent property can not be preserved when the DPPDW NIR rays propagate in the breast tissue. If the simulation model considers the amplitude and phase in the breast tissue, the global SNR should be better and the detectable tumor size would be smaller.","publisher":"World Molecular Imaging Congress","authors":[{"entityUrn":"urn:li:fs_salesProfile:(ACwAACogHXMBJouE-MsI4FuxPKBxiG40paEegPo,NAME_SEARCH,vbiI)"}]},{"name":"Absolute quantitative assessment for micro-SPECT images of 188Re-BMEDA-Liposome in a C26 murine colon carcinoma solid tumor model","publishedOn":{"year":2010},"description":"Micro-SPECT\/CT pre-clinical experiment is a powerful tool for quantifying the distribution of a radioactive compound in living animals. The conventional method of absolute quantification uses a known-activity source as reference, however the reference may cause unexpected crosstalk to the imaging results. In this study, multiple reference radiation sources were scanned to obtain a relation between activity concentrations and pixel values. After restoration being applied on tumor-bearing mice images, it substituted activity\/pixel-value equation with recovered pixel value, finds the more accurate activity quantitation. . And the Lucy Richardson algorithm was used to recover the micro-SPECT mice images. After drawing the region of interest (ROI) on the tumor, activity\/pixel-value equation was used to transform the correlation between real activity concentrations and the mean pixel value of ROI. Standardized uptake values (SUV) of tumor were respectively estimated from original and recovered images with activity\/pixel-value equation and reference source. The results were compared with bio-distribution (5 mice for each time point). It is conducted that the absolute quantitative results with activity\/pixel-value equation can provide better correlation of bio-distribution. The results are expected to benefit the analysis of pre-clinical animal experiments for drug development. ","publisher":"World Molecular Imaging Congress","authors":[{"entityUrn":"urn:li:fs_salesProfile:(ACwAACogHXMBJouE-MsI4FuxPKBxiG40paEegPo,NAME_SEARCH,vbiI)"}]},{"name":"Scatter Feature of a Positron Imager with Dual Plane Geometry","publishedOn":{"year":2014},"description":"Dedicated positron imagers with dual plane detectors for breast get high scatter fraction (SF) due to 3D data acquisition. In this study, we investigate the contribution of scatter from each breast and body outside of FOV, and its effects on lesion visibility. Beam stoppers (BSs) were designed to estimate the scatter distribution. All the data in this study were simulated using GATE. The FOV of this system is 196.8 mm\u00d798.4 mm and the crystal element is 1.64\u00d71.64\u00d710 mm\u00b3 LYSO. A XCAT phantom with F-18-FDG was simulated to obtain the body-scatter contribution to the left\/right breast imaging individually. Besides, the breast phantom without body was simulated to obtain the scatter contribution from each breast. To mimic breast, three lesions with 3, 5, 8 mm diameters were placed inside a 100\u00d7100\u00d780 mm\u00b3 box phantom. Various T\/B ratios were simulated for lesion visibility. Three BSs made of lead were placed between the upper detector and the box phantom. The phantom was scanned twice (w\/wo BS). The BS blocks partially the recording of the primary events without affecting the scatter and random. From the counts of two scans, we can estimate the scatter distribution. The results of XCAT phantom show that the SF of left (right) breast imaging was 33.3% (33.1%) and 30.2% (29.8%), respectively, with and without the inclusion of body. The SF of box phantom was 36.4% which was higher than XCAT phantom due to the larger volume. At the T\/B ratio 3:1 condition, the 3 mm lesion was faintly visible suffered from scatter. The scatter distribution estimated by BS was shown. The results indicate that left and right breast imaging are suffered almost the same scatter effect. The scatter from out of FOV could be ignored. The results of SF values and lesion visibility study show the scatter effect of positron imager with dual plane geometry could not be ignored. The BSs were designed to estimate the scatter distribution, and could be used in scatter correction in the future.","publisher":"IEEE NUCLEAR SCIENCE SYMPOSIUM & MEDICAL IMAGING CONFERENCE","authors":[{"entityUrn":"urn:li:fs_salesProfile:(ACwAACogHXMBJouE-MsI4FuxPKBxiG40paEegPo,NAME_SEARCH,vbiI)"}]},{"name":"Using CUDA to Accelerate Iterative Forward And Backward Processing of Emission Tomographic Reconstruction","publishedOn":{"year":2010},"description":"For medical imaging modality such as positron emission tomography, x-ray computed tomography, etc, iterative MLEM (maximum likelihood expectation maximization) reconstruction is used commonly to obtain better-quality tomographical images. However, it is compute-intensive to process especially for high resolution image. To three-dimensional reconstruction of dual planar PET (figure 1), a CUDA implementation of the reconstruction can obtain significant time reductions while the calculation involves each detector-element pair (lines of response, LOR) and all voxels. In general, performance improvement of GPU computing is quite obvious. After the acceleration, the iterative reconstruction would be more practicable and more attractive in clinical use.","publisher":"Society of Nuclear Medicine, Taiwan(R.O.C.)","authors":[{"entityUrn":"urn:li:fs_salesProfile:(ACwAACogHXMBJouE-MsI4FuxPKBxiG40paEegPo,NAME_SEARCH,vbiI)"}]}],"positions":null,"posts":[{"createdAt":1717254600000,"insightId":"0c873b0a-68f7-4cd0-8bcb-b89abec46697","activityUnion":{"postActivity":{"socialMetadata":{"reactionTypeCounts":[],"entityUrn":"urn:li:fs_salesSocialMetadata:urn:li:ugcPost:7202688053675479040","threadUrn":"urn:li:ugcPost:7202688053675479040","reactionsCount":0,"commentsCount":0,"canComment":true},"activityUrl":"https:\/\/www.linkedin.com\/feed\/sales-navigator\/urn:li:ugcPost:7202688053675479040","message":{"attributes":[{"start":94,"length":5,"value":{"com.linkedin.common.HashtagAttributedEntity":{"hashtag":"urn:li:hashtag:llms"}}},{"start":100,"length":4,"value":{"com.linkedin.common.HashtagAttributedEntity":{"hashtag":"urn:li:hashtag:rag"}}},{"start":105,"length":11,"value":{"com.linkedin.common.HashtagAttributedEntity":{"hashtag":"urn:li:hashtag:llamaindex"}}}],"text":"llamaindex-omakase-rag\n- \u4f7f\u7528 Scheduler\u00a0\u81ea\u52d5\u66f4\u65b0\u4f86\u81ea\u00a0Google Drive \u7684\u6578\u64da\n- \u591a\u4f7f\u7528\u8005\u8a2a\u554f\u548c\u8a31\u53ef\u6b0a\u63a7\u5236\n- RAG \u4ecb\u9762\n- \u7ba1\u7406\u9762\u677f\n\n#LLMs\n#RAG\n#LlamaIndex"},"rootActivity":{"contentSummaryUnion":{"mediaContentSummary":{"mediaType":"VIDEO","thumnailUrl":"https:\/\/dms.licdn.com\/playlist\/vid\/D5605AQEt2LVCw8n7KA\/videocover-firstframe-low\/0\/1717172400685?e=1717977600&v=beta&t=VZyKXskK-crx1Ob1Vj0XCwQo0GpcVF7bwX0rkOH-Yi4"}},"activityUrl":"https:\/\/www.linkedin.com\/feed\/sales-navigator\/urn:li:ugcPost:7202342838632669185","message":{"attributes":[{"length":24,"start":387,"value":{"com.linkedin.common.HyperlinkAttributedEntity":{"url":"https:\/\/lnkd.in\/gaYX7PKA"}}}],"text":"Want help building a full web app using LlamaIndex? Introducing Omakase RAG Orchestrator!\n\nA web app template for building scalable RAG applications with Django, Llamaindex, and Google Drive. Features:\n\u27a1\ufe0f full-featured RAG API\n\u27a1\ufe0f data source management\n\u27a1\ufe0f user access control\n\u27a1\ufe0f an admin panel\n\nCheck out the video introducing the project below, or head over to the repo to learn more: \nhttps:\/\/lnkd.in\/gaYX7PKA"},"entityUrn":"urn:li:ugcPost:7202342838632669185"},"entityUrn":"urn:li:ugcPost:7202688053675479040"}}},{"createdAt":1716963120000,"insightId":"a84a63ef-7ebf-4304-9579-c5f2873a03c5","activityUnion":{"postActivity":{"contentSummaryUnion":{"ingestedContentSummary":{"image":{"attributes":[{"originalImageUrl":"https:\/\/media.licdn.com\/dms\/image\/sync\/D5627AQG0uvmGkC84sw\/articleshare-shrink_800\/0\/1716915301900?e=1717977600&v=beta&t=Tii-IM8VP2yAYdScyW4OMVdufIvJA1Ynw5EzU8G2LEw","sourceType":"URL","imageUrl":"https:\/\/media.licdn.com\/dms\/image\/sync\/D5627AQG0uvmGkC84sw\/articleshare-shrink_800\/0\/1716915301900?e=1717977600&v=beta&t=Tii-IM8VP2yAYdScyW4OMVdufIvJA1Ynw5EzU8G2LEw"}]},"description":"Following the recent popularity of Large Language Models (LLMs), several attempts have been made to extend them to the visual domain. From having a visual assistant that could guide us through unfamiliar environments to generative models that produce...","resolvedUrl":"https:\/\/arxiv.org\/abs\/2405.17247","title":"An Introduction to Vision-Language Modeling"}},"socialMetadata":{"reactionTypeCounts":[{"type":"LIKE","count":3}],"entityUrn":"urn:li:fs_salesSocialMetadata:urn:li:activity:7201465386301628416","threadUrn":"urn:li:activity:7201465386301628416","reactionsCount":3,"commentsCount":0,"canComment":true},"activityUrl":"https:\/\/www.linkedin.com\/feed\/sales-navigator\/urn:li:share:7201465385676623872","message":{"attributes":[{"start":122,"length":5,"value":{"com.linkedin.common.HashtagAttributedEntity":{"hashtag":"urn:li:hashtag:llms"}}},{"start":128,"length":5,"value":{"com.linkedin.common.HashtagAttributedEntity":{"hashtag":"urn:li:hashtag:meta"}}},{"start":134,"length":4,"value":{"com.linkedin.common.HashtagAttributedEntity":{"hashtag":"urn:li:hashtag:vlm"}}}],"text":"An Introduction to Vision-Language Modeling\n\u672c\u6587\u4ecb\u7d39\u4e86\u8996\u89ba\u8a9e\u8a00\u6a21\u578b(VLM)\uff0c\u7d50\u5408\u96fb\u8166\u8996\u89ba\u8207\u81ea\u7136\u8a9e\u8a00\u8655\u7406\u3002\n\u8ad6\u6587\u6db5\u84cb\u4e86\u8a13\u7df4 VLMs \u7684\u65b9\u6cd5\u3001VLMs \u7684\u8a55\u4f30\u65b9\u6cd5\u4ee5\u53ca VLMs \u5728\u5f71\u7247\u4e0a\u7684\u61c9\u7528\u3002\n\n\n#LLMs\n#Meta\n#VLM"},"entityUrn":"urn:li:share:7201465385676623872"}}},{"createdAt":1716903300000,"insightId":"84ba565e-3b1a-4fb9-bb23-72a629d6cac5","activityUnion":{"postActivity":{"contentSummaryUnion":{"ingestedContentSummary":{"image":{"attributes":[{"originalImageUrl":"https:\/\/media.licdn.com\/dms\/image\/sync\/D5627AQG_E79e-VjJMw\/articleshare-shrink_800\/0\/1716892486802?e=1717977600&v=beta&t=cRSA4cViPO4CyK9CiueJu3dw3oISDN12EHx0ORI86-8","sourceType":"URL","imageUrl":"https:\/\/media.licdn.com\/dms\/image\/sync\/D5627AQG_E79e-VjJMw\/articleshare-shrink_800\/0\/1716892486802?e=1717977600&v=beta&t=cRSA4cViPO4CyK9CiueJu3dw3oISDN12EHx0ORI86-8"}]},"publishedAt":1717017607000,"description":"Today, the OpenAI Board formed a Safety and Security Committee led by directors Bret Taylor (Chair), Adam D\u2019Angelo, Nicole Seligman, and Sam Altman (CEO). This committee will be responsible for making recommendations to the full Board on critical...","fullText":"Today, the OpenAI Board formed a Safety and Security Committee led by directors Bret Taylor (Chair), Adam D\u2019Angelo, Nicole Seligman, and Sam Altman (CEO). This committee will be responsible for making recommendations to the full Board on critical safety and security decisions for OpenAI projects and operations. OpenAI has recently begun training its next frontier model and we anticipate the resulting systems to bring us to the next level of capabilities on our path to AGI. While we are proud to build and release models that are industry-leading on both capabilities and safety, we welcome a robust debate at this important moment. A first task of the Safety and Security Committee will be to evaluate and further develop OpenAI\u2019s processes and safeguards over the next 90 days. At the conclusion of the 90 days, the Safety and Security Committee will share their recommendations with the full Board. Following the full Board\u2019s review, OpenAI will publicly share an update on adopted recommendations in a manner that is consistent with safety and security. OpenAI technical and policy experts Aleksander Madry (Head of Preparedness), Lilian Weng (Head of Safety Systems), John Schulman (Head of Alignment Science), Matt Knight (Head of Security), and Jakub Pachocki (Chief Scientist) will also be on the committee. Additionally, OpenAI will retain and consult with other safety, security, and technical experts to support this work, including former cybersecurity officials, Rob Joyce, who advises OpenAI on security, and John Carlin.","resolvedUrl":"https:\/\/openai.com\/index\/openai-board-forms-safety-and-security-committee\/?","title":"OpenAI Board Forms Safety and Security Committee","sourceDomain":"openai.com"}},"socialMetadata":{"reactionTypeCounts":[{"type":"LIKE","count":2}],"entityUrn":"urn:li:fs_salesSocialMetadata:urn:li:activity:7201214487436091392","threadUrn":"urn:li:activity:7201214487436091392","reactionsCount":2,"commentsCount":0,"canComment":true},"activityUrl":"https:\/\/www.linkedin.com\/feed\/sales-navigator\/urn:li:share:7201214486773473280","message":{"attributes":[{"start":91,"length":5,"value":{"com.linkedin.common.HashtagAttributedEntity":{"hashtag":"urn:li:hashtag:llms"}}},{"start":97,"length":7,"value":{"com.linkedin.common.HashtagAttributedEntity":{"hashtag":"urn:li:hashtag:openai"}}}],"text":"OpenAI \u6210\u7acb\u65b0\u7684\u5b89\u5168\u59d4\u54e1\u6703\uff0c\u4e26\u5df2\u958b\u59cb\u8a13\u7df4\u4e0b\u500bAI\u6a21\u578b(OpenAI has recently begun training its next frontier model)\n\n#LLMs\n#OpenAI"},"entityUrn":"urn:li:share:7201214486773473280"}}},{"createdAt":1716902760000,"insightId":"00ab0a6e-fb00-4bbb-8ee3-037982311389","activityUnion":{"postActivity":{"contentSummaryUnion":{"ingestedContentSummary":{"image":{"attributes":[{"originalImageUrl":"https:\/\/media.licdn.com\/dms\/image\/sync\/D4E27AQGa5VWpk9NoaA\/articleshare-shrink_800\/0\/1716896356272?e=1717977600&v=beta&t=NUtH-oYSQb4yjPWoegQjUXkXZCbT1e_EYDv3KGqM_ao","sourceType":"URL","imageUrl":"https:\/\/media.licdn.com\/dms\/image\/sync\/D4E27AQGa5VWpk9NoaA\/articleshare-shrink_800\/0\/1716896356272?e=1717977600&v=beta&t=NUtH-oYSQb4yjPWoegQjUXkXZCbT1e_EYDv3KGqM_ao"}]},"description":"Retrieval-augmented generation (RAG) has been shown vulnerable to retrieval corruption attacks: an attacker can inject malicious passages into retrieval results to induce inaccurate responses. In this paper, we propose RobustRAG as the first defense...","resolvedUrl":"https:\/\/arxiv.org\/abs\/2405.15556","title":"Certifiably Robust RAG against Retrieval Corruption"}},"socialMetadata":{"reactionTypeCounts":[{"type":"LIKE","count":4}],"entityUrn":"urn:li:fs_salesSocialMetadata:urn:li:activity:7201212155436703744","threadUrn":"urn:li:activity:7201212155436703744","reactionsCount":4,"commentsCount":0,"canComment":true},"activityUrl":"https:\/\/www.linkedin.com\/feed\/sales-navigator\/urn:li:share:7201212154966937600","message":{"attributes":[{"start":376,"length":5,"value":{"com.linkedin.common.HashtagAttributedEntity":{"hashtag":"urn:li:hashtag:llms"}}}],"text":"Certifiably Robust RAG against Retrieval Corruption\nRobustRAG \u7684\u9632\u79a6\u6846\u67b6\uff0c\u80fd\u9632\u6b62\u7576\u653b\u64ca\u8005\u5c07\u60e1\u610f\u6bb5\u843d\u6ce8\u5165\u77e5\u8b58\u5eab\uff0c\u8a98\u4f7f RAG \u751f\u6210\u932f\u8aa4\u7684\u56de\u61c9\u3002\nRobustRAG \u7684\u95dc\u9375\u662f\u63a1\u7528\u300c\u9694\u96e2\u5f8c\u805a\u5408\u300d\u7b56\u7565\uff1a\n\u5148\u5c07\u6bcf\u500b\u6bb5\u843d\u9694\u96e2\uff0c\u4e26\u500b\u5225\u5730\u8b93 LLM \u751f\u6210\u56de\u61c9\uff0c\u7136\u5f8c\u518d\u5b89\u5168\u5730\u5c07\u9019\u4e9b\u9694\u96e2\u7684\u56de\u61c9\u805a\u5408\u8d77\u4f86\u3002\u9019\u7a2e\u9694\u96e2\u7b56\u7565\u53ef\u4ee5\u78ba\u4fdd\u60e1\u610f\u6bb5\u843d\u4e0d\u6703\u5f71\u97ff\u5176\u4ed6\u826f\u6027\u6bb5\u843d\u7684 LLM \u56de\u61c9\uff0c\u5f9e\u800c\u70ba RobustRAG \u7684\u5f37\u5065\u6027\u5960\u5b9a\u4e86\u57fa\u790e\u3002\n\n\u57f7\u884c\u6b65\u9a5f\u4e2d\uff0c\u9700\u5c0d\u6240\u6709\u53ef\u80fd\u7684\u56de\u61c9\u9032\u884c\u8a55\u4f30\uff0c\u611f\u89ba\u9019\u6a23\u8d85\u8017Token\u3002\u5c0d\u5927\u578b\u77e5\u8b58\u5eab\u548c\u8907\u96dc\u7684\u67e5\u8a62\uff0c\u9019\u7a2e\u6210\u672c\u53ef\u80fd\u904e\u9ad8\u3002\n\u53e6\u5916\u8a2d\u8a08\u770b\u4f86\u50c5\u9069\u7528\u65bc\u55ae\u8df3RAG\uff0c\u5c0d\u65bc\u591a\u8df3RAG(MultiHop-RAG\n)\u9700\u8981\u62c6\u5206\u6210\u591a\u500b\u55ae\u8df3RAG\u624d\u80fd\u61c9\u7528\u3002\n\u6700\u5f8cRobustRAG\u5de5\u4f5c\u6a5f\u5236\u904e\u65bc\u8907\u96dc\uff0c\u4e0d\u5229\u65bc\u9032\u884c\u53ef\u89e3\u91cb\u6027\u5206\u6790\u3002\n\n#LLMs"},"entityUrn":"urn:li:share:7201212154966937600"}}},{"createdAt":1716899820000,"insightId":"f369513c-1252-4857-8ca6-cea3679d9d69","activityUnion":{"postActivity":{"contentSummaryUnion":{"ingestedContentSummary":{"image":{"attributes":[{"originalImageUrl":"https:\/\/media.licdn.com\/dms\/image\/sync\/D4E27AQFJSJtNWmr8lA\/articleshare-shrink_800\/0\/1716890151852?e=1717977600&v=beta&t=xXstoXxcUKAzxF8Yw0MwrOHhk6SBESiE4S_vSavBc4E","sourceType":"URL","imageUrl":"https:\/\/media.licdn.com\/dms\/image\/sync\/D4E27AQFJSJtNWmr8lA\/articleshare-shrink_800\/0\/1716890151852?e=1717977600&v=beta&t=xXstoXxcUKAzxF8Yw0MwrOHhk6SBESiE4S_vSavBc4E"}]},"description":"Testing two families of large language models (LLMs) (GPT and LLaMA2) on a battery of measurements spanning different theory of mind abilities, Strachan et al. find that the performance of LLMs can mirror that of humans on most of these tasks. The...","resolvedUrl":"https:\/\/www.nature.com\/articles\/s41562-024-01882-z","title":"Testing theory of mind in large language models and humans - Nature Human Behaviour"}},"socialMetadata":{"reactionTypeCounts":[{"type":"LIKE","count":1}],"entityUrn":"urn:li:fs_salesSocialMetadata:urn:li:activity:7201199856353230848","threadUrn":"urn:li:activity:7201199856353230848","reactionsCount":1,"commentsCount":0,"canComment":true},"activityUrl":"https:\/\/www.linkedin.com\/feed\/sales-navigator\/urn:li:share:7201199855711473664","message":{"attributes":[{"start":510,"length":24,"value":{"com.linkedin.common.HyperlinkAttributedEntity":{"url":"https:\/\/lnkd.in\/gE_9jhie"}}},{"start":537,"length":5,"value":{"com.linkedin.common.HashtagAttributedEntity":{"hashtag":"urn:li:hashtag:llms"}}}],"text":"Testing theory of mind in large language models and humans\n\n\u672c\u6587\u737b\u4e2d\u63a2\u8a0e\u4e86\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u7406\u89e3\u4ed6\u4eba\u5fc3\u667a\uff08theory of mind\uff09\u65b9\u9762\u7684\u80fd\u529b\uff0c\u4e26\u8207\u4eba\u985e\u9032\u884c\u6bd4\u8f03\u3002\u7814\u7a76\u4eba\u54e1\u4f7f\u7528\u4e86\u4e00\u7cfb\u5217\u5fc3\u7406\u6e2c\u9a57\u4f86\u8a55\u4f30 LLMs \u5728\u7406\u89e3\u865b\u5047\u4fe1\u5ff5\u3001\u9593\u63a5\u8acb\u6c42\u3001\u8af7\u523a\u548c\u5931\u614b\u7b49\u65b9\u9762\u7684\u8868\u73fe\u3002\n\n\u7814\u7a76\u7d50\u679c\u986f\u793a\uff0cGPT-4 \u6a21\u578b\u5728\u8b58\u5225\u9593\u63a5\u8acb\u6c42\u3001\u865b\u5047\u4fe1\u5ff5\u548c\u8aa4\u5c0e\u65b9\u9762\u8868\u73fe\u51fa\u8272\uff0c\u751a\u81f3\u8d85\u8d8a\u4e86\u4eba\u985e\u3002\u7136\u800c\uff0cGPT-4 \u5728\u8b58\u5225\u5931\u614b\u65b9\u9762\u8868\u73fe\u4e0d\u4f73\u3002\u53e6\u4e00\u65b9\u9762\uff0cLLaMA2 \u6a21\u578b\u5728\u5931\u614b\u6e2c\u8a66\u4e2d\u8868\u73fe\u512a\u65bc\u4eba\u985e\uff0c\u4f46\u5f8c\u7e8c\u7684\u64cd\u63a7\u5be6\u9a57\u986f\u793a\uff0cLLaMA2 \u7684\u512a\u52e2\u53ef\u80fd\u662f\u7531\u65bc\u5176\u50be\u5411\u65bc\u5c07\u7121\u77e5\u6b78\u548e\u65bc\u4ed6\u4eba\u3002\n\n\u7814\u7a76\u4eba\u54e1\u63a8\u6e2c\uff0cGPT \u6a21\u578b\u5728\u5931\u614b\u6e2c\u8a66\u4e2d\u8868\u73fe\u4e0d\u4f73\u53ef\u80fd\u662f\u7531\u65bc\u5176\u904e\u65bc\u4fdd\u5b88\u7684\u7b56\u7565\uff0c\u5c0e\u81f4\u5176\u4e0d\u9858\u5728\u7f3a\u4e4f\u5145\u5206\u8b49\u64da\u7684\u60c5\u6cc1\u4e0b\u505a\u51fa\u7d50\u8ad6\u3002\u9019\u8207 GPT \u6a21\u578b\u5728\u8655\u7406\u793e\u6703\u4e0d\u78ba\u5b9a\u6027\u65b9\u9762\u7684\u7368\u7279\u65b9\u5f0f\u6709\u95dc\uff0c\u800c\u9019\u7a2e\u65b9\u5f0f\u8207\u4eba\u985e\u7684\u884c\u70ba\u6a21\u5f0f\u4e0d\u540c\u3002\n\n\u9019\u9805\u7814\u7a76\u5f37\u8abf\u4e86\u5c0d LLMs \u9032\u884c\u7cfb\u7d71\u6027\u6e2c\u8a66\u548c\u9a57\u8b49\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u78ba\u4fdd\u5c0d\u5176\u80fd\u529b\u7684\u8a55\u4f30\u5168\u9762\u4e14\u6e96\u78ba\u3002\u6b64\u5916\uff0c\u7814\u7a76\u4eba\u54e1\u547c\u7c72\u516c\u958b LLMs \u7684\u53c3\u6578\u3001\u6578\u64da\u548c\u6587\u4ef6\uff0c\u4ee5\u4fbf\u7814\u7a76\u4eba\u54e1\u53ef\u4ee5\u66f4\u6df1\u5165\u5730\u7814\u7a76\u9019\u4e9b\u6a21\u578b\u7684\u793e\u6703\u63a8\u7406\u80fd\u529b\u3002\n\n\u7c21\u4e2d\u7ffb\u8b6f\nhttps:\/\/lnkd.in\/gE_9jhie\n\n\n#LLMs"},"entityUrn":"urn:li:share:7201199855711473664"}}},{"createdAt":1716727560000,"insightId":"bd20952b-c2bf-4715-910d-59fcf36baba9","activityUnion":{"postActivity":{"contentSummaryUnion":{"ingestedContentSummary":{"image":{"attributes":[{"originalImageUrl":"https:\/\/media.licdn.com\/dms\/image\/sync\/D4D27AQGA917dDNd9xw\/articleshare-shrink_800\/0\/1716714714530?e=1717977600&v=beta&t=xB9OFS0QeIeZUrhBLUP6tHFSmw9OwQY7V4yAnHOmIZc","sourceType":"URL","imageUrl":"https:\/\/media.licdn.com\/dms\/image\/sync\/D4D27AQGA917dDNd9xw\/articleshare-shrink_800\/0\/1716714714530?e=1717977600&v=beta&t=xB9OFS0QeIeZUrhBLUP6tHFSmw9OwQY7V4yAnHOmIZc"}]},"description":"YOLOv10: Real-Time End-to-End Object Detection. Contribute to THU-MIG\/yolov10 development by creating an account on GitHub.","resolvedUrl":"https:\/\/github.com\/THU-MIG\/yolov10","title":"GitHub - THU-MIG\/yolov10: YOLOv10: Real-Time End-to-End Object Detection"}},"socialMetadata":{"reactionTypeCounts":[{"type":"LIKE","count":6}],"entityUrn":"urn:li:fs_salesSocialMetadata:urn:li:activity:7200477497631649792","threadUrn":"urn:li:activity:7200477497631649792","reactionsCount":6,"commentsCount":1,"canComment":true},"activityUrl":"https:\/\/www.linkedin.com\/feed\/sales-navigator\/urn:li:share:7200477497107374080","message":{"attributes":[{"start":24,"length":24,"value":{"com.linkedin.common.HyperlinkAttributedEntity":{"url":"https:\/\/lnkd.in\/g_WMwuW9"}}},{"start":62,"length":24,"value":{"com.linkedin.common.HyperlinkAttributedEntity":{"url":"https:\/\/lnkd.in\/gXPWhHJP"}}},{"start":88,"length":5,"value":{"com.linkedin.common.HashtagAttributedEntity":{"hashtag":"urn:li:hashtag:yolo"}}}],"text":"yolo\u539f\u4f86\u5df2\u7d93\u9ed8\u9ed8\u7684\u5230v10\u4e86\n\nPaper\nhttps:\/\/lnkd.in\/g_WMwuW9\n\nOnline demo\nhttps:\/\/lnkd.in\/gXPWhHJP\n\n#yolo"},"entityUrn":"urn:li:share:7200477497107374080"}}},{"createdAt":1716642600000,"insightId":"3f625678-ddea-493c-86e1-cd2f5cad03d2","activityUnion":{"postActivity":{"contentSummaryUnion":{"ingestedContentSummary":{"image":{"attributes":[{"originalImageUrl":"https:\/\/media.licdn.com\/dms\/image\/sync\/D4E27AQGRZYZ7hXUzsg\/articleshare-shrink_800\/0\/1716895917144?e=1717977600&v=beta&t=F2DcPA00hD4YMR9qAxNKm21VeKJMkK0Hlm2ZsT9HU3Y","sourceType":"URL","imageUrl":"https:\/\/media.licdn.com\/dms\/image\/sync\/D4E27AQGRZYZ7hXUzsg\/articleshare-shrink_800\/0\/1716895917144?e=1717977600&v=beta&t=F2DcPA00hD4YMR9qAxNKm21VeKJMkK0Hlm2ZsT9HU3Y"}]},"publishedAt":1716620987000,"description":"Contribute to mistralai\/mistral-finetune development by creating an account on GitHub.","fullText":"is a light-weight codebase that enables memory-efficient and performant finetuning of Mistral's models. It is based on LoRA , a training paradigm where most weights are frozen and only 1-2% additional weights in the form of low-rank matrix perturbations are trained. For maximum efficiency it is recommended to use a A100 or H100 GPU. The codebase is optimized for multi-GPU-single-node training setups, but for smaller models, such as the 7B a single GPU suffices. To get started with Mistral LoRA fine-tuning, follow these steps: We recommend fine-tuning one of the official Mistral models which you can download here: Important Notice : For 8x7B Base V1 and 8x7B Instruct V1, it is necessary to use our v3 tokenizer and extend the vocabulary size to 32768 prior to fine-tuning. For detailed instructions on this process, please refer to the \"Model extension\" section. E.g., to download the 7B-base model you can run the following command: mkdir -p ~ \/ ${HOME} \/mistral_models cd ${HOME} && wget https:\/\/models.mistralcdn.com\/mistral-7b-v0-3\/mistral-7B-v0.3.tar tar -xf mistral-7B-v0.3.tar -C mistral_models Make sure to modify your training script and add the path to the downloaded folder as . E.g., modify example\/7B.yaml to include the absolute path to : To ensure effective training, has strict requirements for how the training data has to be formatted. All data files must be stored in jsonl format files. You can build two types of data files: Pretrain data corresponds to plain text data stored in the key. E.g: Currently two different types of instruction following data are supported: { \"messages\" : [ { \"role\" : \" user \" , \"content\" : \" User interaction n\u00b01 contained in document n\u00b01 \" }, { \"role\" : \" assistant \" , \"content\" : \" Bot interaction n\u00b01 contained in document n\u00b01 \" }, { \"role\" : \" user \" , \"content\" : \" User interaction n\u00b02 contained in document n\u00b01 \" }, { \"role\" : \" assistant \" , \"content\" : \" Bot interaction n\u00b02 contained in document n\u00b01 \" } ] } { \"messages\" : [ { \"role\" : \" user \" , \"content\" : \" User interaction n\u00b01 contained in document n\u00b02 \" }, { \"role\" : \" assistant \" , \"content\" : \" Bot interaction n\u00b01 contained in document n\u00b02 \" }, { \"role\" : \" user \" , \"content\" : \" User interaction n\u00b02 contained in document n\u00b02 \" }, { \"role\" : \" assistant \" , \"content\" : \" Bot interaction n\u00b02 contained in document n\u00b02 \" , \"weight\" : 0 , # don't train on n\u00b02 }, { \"role\" : \" user \" , \"content\" : \" User interaction n\u00b03 contained in document n\u00b02 \" }, { \"role\" : \" assistant \" , \"content\" : \" Bot interaction n\u00b03 contained in document n\u00b02 \" } ] } Note : In function calling the of and the are randomly generated strings of exactly 9 chars. We recommend to generate this automatically in a data preparation script as is done here . { \"messages\" : [ { \"role\" : \" system \" , \"content\" : \" You are an helpful assistant who has access to the following functions to help the user, you can use the functions if needed \" }, { \"role\" : \" user \" , \"content\" : \" Can you help me generate an anagram of the word \\\" listen \\\" ? \" }, { \"role\" : \" assistant \" , \"tool_calls\" : [ { \"id\" : \" TX92Jm8Zi \" , \"type\" : \" function \" , \"function\" : { \"name\" : \" generate_anagram \" , \"arguments\" : \" { \\\" word \\\" : \\\" listen \\\" } \" } } ] }, { \"role\" : \" tool \" , \"content\" : \" { \\\" anagram \\\" : \\\" silent \\\" } \" , \"tool_call_id\" : \" TX92Jm8Zi \" }, { \"role\" : \" assistant \" , \"content\" : \" The anagram of the word \\\" listen \\\" is \\\" silent \\\" . \" }, { \"role\" : \" user \" , \"content\" : \" That's amazing! Can you generate an anagram for the word \\\" race \\\" ? \" }, { \"role\" : \" assistant \" , \"tool_calls\" : [ { \"id\" : \" 3XhQnxLsT \" , \"type\" : \" function \" , \"function\" : { \"name\" : \" generate_anagram \" , \"arguments\" : \" { \\\" word \\\" : \\\" race \\\" } \" } } ] } ], \"tools\" : [ { \"type\" : \" function \" , \"function\" : { \"name\" : \" generate_anagram \" , \"description\" : \" Generate an anagram of a given word \" , \"parameters\" : { \"type\" : \" object \" , \"properties\" : { \"word\" : { \"type\" : \" string \" , \"description\" : \" The word to generate an anagram of \" } }, \"required\" : [ \" word \" ] } } } ] } Before starting a training run you should verify that your dataset is correctly formatted and get an estimation of the training time. You can do so by using the .\/utils\/validate_data script. Note that this step is crucial to ensure that the data is correctly formatted. Let's go over a simple example to train a model in instruction following: Create the data folder and navigate to the folder. Load the data into a Pandas Dataframe. Note : Make sure to have pandas and pyarrow installed ( ). import pandas as pd df = pd . read_parquet ( 'https:\/\/huggingface.co\/datasets\/HuggingFaceH4\/ultrachat_200k\/resolve\/main\/data\/test_gen-00000-of-00001-3d4cd8309148a71f.parquet' ) Modify example\/7B.yaml to include the absolute path to as well as a dataset mixing weight for training and for eval, e.g. Now you can verify your training yaml to make sure the data is correctly formatted and to get an estimate of your training time. Upon completion you should see an error report with many of the following errors: Many conversations seem to end with the 'user' role which is unnecessary as we only train on 'assistant' messages and thus would unnecessarily process data. You should see that a couple of samples will be skipped. Upon correction of the dataset, run the script again You should get a summary of the data input and training parameters: Having set to 500 would lead to iterating through the dataset roughly 5 times which is reasonable, but might be a bit too much. A recommended setting is shown below which would only take 30min on a 8xH100 cluster. Next let's go over a more advanced use case to fine-tune a model on function calling. Function calling requires the data to be in the format as explained above . Let's go over an example. Create the data folder and navigate to the folder. Load the data into a Pandas Dataframe. Note : Make sure to have pandas and pyarrow installed ( ). import pandas as pd df = pd . read_parquet ( 'https:\/\/huggingface.co\/datasets\/Locutusque\/function-calling-chatml\/resolve\/main\/data\/train-00000-of-00001-f0b56c6983b4a78f.parquet' ) As one can see the dataset does not follow the required function calling format, so it will need to be reformatted. Among other things should be renamed to and superfluous characters should be removed. For this dataset you can make use of : Running this command will make sure that most samples are in the correct format. Note : It is impossible to write reformatting scripts that work for all kinds of datasets. If you have datasets that don't yet follow the required format above, you will most probably have to create a reformatting script yourself (mistral-chat or chat-gpt is your best friend here!). You can now validate the dataset by setting and to and in respectively. The reformatted datasets still has some errors which can be removed with . For this, make sure to add as follows: Running this command will show a couple of errors and save two new datasets and . Make sure to use these two dataset in and run the command again. Now the dataset should be correctly formatted! Having followed the dataset verification section , we can now start training. For faster training, we recommend setting max_steps to only 300. Make sure to define to your experiment folder and optionally set to a Weights & Biases project for logging`, e.g. : Save the training configuration and start training! Make sure to set to the number of available GPUs. Training on ultra-chat should take around 30min on a 8xH100 node and the resulting weights should give an MT Bench score around 6.3. Training on glaive should take around 1h on a 8xH100 node and the resulting weights should work nicely for function calling. The example defines reasonable parameters for learning rate, weight decay, etc... but you are advised to customize these settings for your use case. Generally, a training configuration should fill the following parameters: Once your model is trained, you should try it out in inference. We recommend using mistral-inference . Make sure to have correctly installed: Assuming your is saved under , you can chat with the model using , e.g. : mistral-chat \/mnt\/slow\/runs\/patrick\/mistral-finetune\/7B\/ --max_tokens 256 --temperature 1.0 --instruct --lora_path $HOME \/ultra_chat_test\/checkpoints\/checkpoint_000300\/consolidated\/lora.safetensors Important : Note that one can only fine-tune mistral models that are compatible with the v3 tokenizer which entails that the models have a vocabulary size of 32768 - not 32000. One can however easily extend older version of vocabulary size 32000 to have a vocabulary size of 32768 by using: Once the extension has worked, one can fine-tune using the newly created model checkpoint in . We see a higher degree of performance variance in when fine-tuning MoE models. It's not unusual to find that fine-tuning MoEs models with different seeds can lead to a high variance in performance. We did not observe such a high variance with dense models. Therefore, we suggest running multiple instances of the same fine-tuning process on MoEs models and selecting the one that performs best. You can use the following script to find out: https:\/\/github.com\/mistralai\/mistral-finetune\/blob\/main\/utils\/validate_data.py . This script accepts a .yaml training file as input and returns the number of tokens the model is being trained on. One possible solution is to reduce the batch size per GPU. The batch size is equal to x . Try setting to 1 and reduce . You can define the and in the .yaml file.","resolvedUrl":"https:\/\/github.com\/mistralai\/mistral-finetune","title":"GitHub - mistralai\/mistral-finetune","sourceDomain":"github.com"}},"socialMetadata":{"reactionTypeCounts":[{"type":"LIKE","count":3}],"entityUrn":"urn:li:fs_salesSocialMetadata:urn:li:activity:7200121088268263424","threadUrn":"urn:li:activity:7200121088268263424","reactionsCount":3,"commentsCount":0,"canComment":true},"activityUrl":"https:\/\/www.linkedin.com\/feed\/sales-navigator\/urn:li:share:7200121087714631680","message":{"attributes":[{"start":27,"length":5,"value":{"com.linkedin.common.HashtagAttributedEntity":{"hashtag":"urn:li:hashtag:llms"}}},{"start":33,"length":8,"value":{"com.linkedin.common.HashtagAttributedEntity":{"hashtag":"urn:li:hashtag:mistral"}}}],"text":"Mistral \u767c\u5e03\u5176\u5c08\u7528\u5fae\u8abf\u5de5\u5177(\u57fa\u65bcLoRA)\n\n#LLMs\n#Mistral\n"},"entityUrn":"urn:li:share:7200121087714631680"}}},{"createdAt":1716214020000,"insightId":"206abcba-ecf1-4038-a3ea-108d5ec7c1a9","activityUnion":{"postActivity":{"contentSummaryUnion":{"ingestedContentSummary":{"image":{"attributes":[{"originalImageUrl":"https:\/\/media.licdn.com\/dms\/image\/sync\/D5627AQF1hAlbI1NfbQ\/articleshare-shrink_800\/0\/1716213939138?e=1717977600&v=beta&t=TsxhpfbM4NGTPG5TZr64s1l25IrMku1RmCJ-bRVeIZ0","sourceType":"URL","imageUrl":"https:\/\/media.licdn.com\/dms\/image\/sync\/D5627AQF1hAlbI1NfbQ\/articleshare-shrink_800\/0\/1716213939138?e=1717977600&v=beta&t=TsxhpfbM4NGTPG5TZr64s1l25IrMku1RmCJ-bRVeIZ0"}]},"description":"A collection of prompts to challenge the reasoning abilities of large language models - cpldcpu\/MisguidedAttention","resolvedUrl":"https:\/\/github.com\/cpldcpu\/MisguidedAttention","title":"GitHub - cpldcpu\/MisguidedAttention: A collection of prompts to challenge the reasoning abilities of large language models"}},"socialMetadata":{"reactionTypeCounts":[{"type":"LIKE","count":10}],"entityUrn":"urn:li:fs_salesSocialMetadata:urn:li:activity:7198323472207405056","threadUrn":"urn:li:activity:7198323472207405056","reactionsCount":10,"commentsCount":0,"canComment":true},"activityUrl":"https:\/\/www.linkedin.com\/feed\/sales-navigator\/urn:li:share:7198323471628574720","message":{"attributes":[{"start":245,"length":5,"value":{"com.linkedin.common.HashtagAttributedEntity":{"hashtag":"urn:li:hashtag:llms"}}}],"text":"\u6311\u6230LLM\u63a8\u7406\u80fd\u529b\u7684\u8aa4\u5c0e\u6027\u554f\u984c\n\u984c\u76ee\u4fee\u6539\u81ea\u77e5\u540d\u601d\u60f3\u5be6\u9a57\u6216\u6096\u8ad6\uff0c\u8b93\u4eba\u985e\u5341\u5206\u5bb9\u6613\u56de\u7b54\uff0c\u800cLLM\u537b\u6703\u53d7\u5230\u5f71\u97ff\uff0c\u9032\u884c\u975e\u5fc5\u8981\u7684\u63a8\u7406\u3002\n\u90e8\u4efdLLM\u53ef\u900f\u904e\u7c21\u55ae\u7684Prompt\u6216\u8005CoT(Chain of Thought)\uff0c\u4f7f\u539f\u672c\u56de\u7b54\u932f\u8aa4\u7684\uff0c\u7372\u5f97\u6b63\u78ba\u7684\u89e3\u7b54\u3002\n\n\u7c21\u55ae\u7684Prompt\u4f8b\u5b50\uff0c\u8981\u6c42\u56de\u7b54\u524d\u91cd\u8907\u554f\u984c\nHi chat, can you answer this question for me? \"{Question}\"Repeat the question before answering it.\n\n\n#LLMs"},"entityUrn":"urn:li:share:7198323471628574720"}}},{"createdAt":1716208380000,"insightId":"7cae8aa3-0518-4113-9e62-dd1d799e3b44","activityUnion":{"postActivity":{"contentSummaryUnion":{"ingestedContentSummary":{"image":{"attributes":[{"originalImageUrl":"https:\/\/media.licdn.com\/dms\/image\/sync\/D4E27AQEHVwCNpdDv7g\/articleshare-shrink_800\/0\/1715958479267?e=1717977600&v=beta&t=gZTasvCgfxAm1KrgKzfljEJejFAOZgO5skk5dQBWdEw","sourceType":"URL","imageUrl":"https:\/\/media.licdn.com\/dms\/image\/sync\/D4E27AQEHVwCNpdDv7g\/articleshare-shrink_800\/0\/1715958479267?e=1717977600&v=beta&t=gZTasvCgfxAm1KrgKzfljEJejFAOZgO5skk5dQBWdEw"}]},"description":"Meta introduced Chameleon, a new multimodal model that seamlessly processes text and images in a unified token space. It could be a precursor to a GPT-4o alternative.","fullText":"Meta introduced Chameleon, a new multimodal model that seamlessly processes text and images in a unified token space. It could be a precursor to a GPT-4o alternative. Meta AI introduced Chameleon, a new approach to training multimodal foundation models that process both text and images as discrete tokens. Unlike previous methods, Chameleon uses a unified transformer architecture and forgoes separate encoders or decoders for different modalities, as employed by other architectures like Unified-IO 2 . Meta's model is trained from the ground up with a mix of text, images, and code. The images are first quantized into discrete tokens that can be processed analogously to words in text. This \"early-fusion\" approach, where all modalities are projected into a common representation space from the beginning, allows Chameleon to seamlessly reason and generate across modalities. However, this poses significant technical challenges for the researchers, particularly in terms of training stability and scalability. To overcome these challenges, the team introduces a series of architectural innovations and training techniques. They also demonstrate how the supervised finetuning methods used for pure language models can be transferred to the mixed-modal case. Using these techniques, the team successfully trained the 34 billion parameter Chameleon model with 10 trillion multimodal tokens - five times more than the pure text model Llama-2. In comparison, the language model Llama-3 was trained with 15 trillion text tokens, so future versions of Chameleon will likely be trained with significantly more tokens. Image: Meta Extensive evaluations show that Chameleon is a versatile model for a wide range of tasks. The 34 billion model achieves top performance in visual question answering and image captioning, surpassing models like Flamingo , IDEFICS, and Llava-1.5, and approaching GPT-4V . At the same time, it remains competitive in pure text tasks, achieving similar performance to Mixtral 8x7B and Gemini-Pro in common sense and reading comprehension tests. However, the most interesting aspect is the entirely new capabilities Chameleon offers in mixed-modal inference and generation. In one test, Meta shows that human evaluators prefer the 34 billion model to Gemini-Pro and GPT-4V in terms of the quality of mixed-modal responses to open-ended questions, i.e., questions that mix images and text. It can also answer questions that include text and generated images. Image: Meta Although little is known about the specific architecture of the recently introduced GPT-4 omni (GPT-4o) from OpenAI, the company is likely pursuing a similar approach. However, in contrast to Chameleon, OpenAI's model also directly incorporates audio, is presumably significantly larger, and generally trained with much more data. According to Armen Aghajanyan, an involved AI researcher at Meta, Chameleon is only the beginning of Meta's work to share knowledge about the next paradigm of scale: \"Early-fusion\" multimodal models are the future. The researcher also noted that the model was trained five months ago and the team has made great progress since then. Integrating additional modalities could be one of them. Meta CEO Mark Zuckerberg has a lready announced multimodal models for the future . Chameleon is not yet available.","resolvedUrl":"https:\/\/the-decoder.com\/metas-chameleon-ai-model-blends-text-and-images-hinting-at-a-future-gpt-4o-rival\/","title":"Meta's Chameleon AI model blends text and images, hinting at a future GPT-4o rival","sourceDomain":"the-decoder.com"}},"socialMetadata":{"reactionTypeCounts":[{"type":"LIKE","count":2}],"entityUrn":"urn:li:fs_salesSocialMetadata:urn:li:activity:7198299877913628672","threadUrn":"urn:li:activity:7198299877913628672","reactionsCount":2,"commentsCount":0,"canComment":true},"activityUrl":"https:\/\/www.linkedin.com\/feed\/sales-navigator\/urn:li:share:7198299877322235904","message":{"attributes":[{"start":76,"length":24,"value":{"com.linkedin.common.HyperlinkAttributedEntity":{"url":"https:\/\/lnkd.in\/gAZH43nc"}}},{"start":108,"length":24,"value":{"com.linkedin.common.HyperlinkAttributedEntity":{"url":"https:\/\/lnkd.in\/g55miJwA"}}},{"start":134,"length":5,"value":{"com.linkedin.common.HashtagAttributedEntity":{"hashtag":"urn:li:hashtag:llms"}}},{"start":140,"length":5,"value":{"com.linkedin.common.HashtagAttributedEntity":{"hashtag":"urn:li:hashtag:meta"}}}],"text":"Chameleon\nMeta\u63a8\u51fa\u5c0d\u6a19OpenAI\u7684GPT-4o\u7684\u539f\u751f\u591a\u6a21\u614b\uff0c\u4e0d\u904e\u76ee\u524d\u4e3b\u8981\u662f\u5f71\u50cf\u8207\u6587\u5b57\uff0c\u7f3a\u5c11\u8a9e\u97f3\u80fd\u529b\uff0c\u5c1a\u672a\u516c\u4f48\u662f\u5426\u958b\u6e90\u3002\n\nPaper:\nhttps:\/\/lnkd.in\/gAZH43nc\n\n\u7c21\u4e2d\u4ecb\u7d39\uff1a\nhttps:\/\/lnkd.in\/g55miJwA\n\n#LLMs\n#Meta"},"entityUrn":"urn:li:share:7198299877322235904"}}},{"createdAt":1715960400000,"insightId":"96c4638f-acf2-4ca2-9c01-5834400a3fba","activityUnion":{"postActivity":{"contentSummaryUnion":{"ingestedContentSummary":{"image":{"attributes":[{"originalImageUrl":"https:\/\/media.licdn.com\/dms\/image\/sync\/D4E27AQH_CCqIC17hwA\/articleshare-shrink_800\/0\/1715956287180?e=1717977600&v=beta&t=T_YJu_OLIhantvJpu9dwP8A_IXyJMG2GYgRqDGcAAkA","sourceType":"URL","imageUrl":"https:\/\/media.licdn.com\/dms\/image\/sync\/D4E27AQH_CCqIC17hwA\/articleshare-shrink_800\/0\/1715956287180?e=1717977600&v=beta&t=T_YJu_OLIhantvJpu9dwP8A_IXyJMG2GYgRqDGcAAkA"}]},"description":"Researchers are striving to reverse-engineer artificial intelligence and scan the \u2018brains\u2019 of LLMs to see what they are doing, how and why.","resolvedUrl":"https:\/\/www.nature.com\/articles\/d41586-024-01314-y","title":"How does ChatGPT \u2018think\u2019? Psychology and neuroscience crack open AI large language models"}},"socialMetadata":{"reactionTypeCounts":[{"type":"LIKE","count":9}],"entityUrn":"urn:li:fs_salesSocialMetadata:urn:li:activity:7197259584112123904","threadUrn":"urn:li:activity:7197259584112123904","reactionsCount":9,"commentsCount":0,"canComment":true},"activityUrl":"https:\/\/www.linkedin.com\/feed\/sales-navigator\/urn:li:share:7197259583373889536","message":{"attributes":[{"start":37,"length":24,"value":{"com.linkedin.common.HyperlinkAttributedEntity":{"url":"https:\/\/lnkd.in\/gUTuREYG"}}},{"start":63,"length":5,"value":{"com.linkedin.common.HashtagAttributedEntity":{"hashtag":"urn:li:hashtag:llms"}}},{"start":69,"length":7,"value":{"com.linkedin.common.HashtagAttributedEntity":{"hashtag":"urn:li:hashtag:nature"}}},{"start":77,"length":8,"value":{"com.linkedin.common.HashtagAttributedEntity":{"hashtag":"urn:li:hashtag:chatgpt"}}}],"text":"chatGPT\u662f\u5982\u4f55\u601d\u8003\u7684\nNature\u4e0a\u7684\u79d1\u666e\u6587\u7ae0\uff0c\u7c21\u4e2d\u7ffb\u8b6f\u9023\u7d50\u5982\u4e0b\uff1a\nhttps:\/\/lnkd.in\/gUTuREYG\n\n#LLMs\n#Nature\n#chatGPT"},"entityUrn":"urn:li:share:7197259583373889536"}}},{"createdAt":1715959740000,"insightId":"224c07f2-6b3d-4c50-a629-c8bb433162dc","activityUnion":{"postActivity":{"contentSummaryUnion":{"ingestedContentSummary":{"image":{"attributes":[{"originalImageUrl":"https:\/\/media.licdn.com\/dms\/image\/sync\/D5627AQH6FIOhWHPmXQ\/articleshare-shrink_800\/0\/1715896828028?e=1717977600&v=beta&t=QV3ra8lEUYVEse_q8luHNXGEjr_E1wO_79j0-ODG4NU","sourceType":"URL","imageUrl":"https:\/\/media.licdn.com\/dms\/image\/sync\/D5627AQH6FIOhWHPmXQ\/articleshare-shrink_800\/0\/1715896828028?e=1717977600&v=beta&t=QV3ra8lEUYVEse_q8luHNXGEjr_E1wO_79j0-ODG4NU"}]},"publishedAt":1715897188000,"description":"Improvements to data analysis in ChatGPT Interact with tables and charts and add files directly from Google Drive and Microsoft OneDrive.","fullText":"When you add a dataset, ChatGPT will create an interactive table that you can expand to a full-screen view so you can follow along as it updates during your analysis. Click on specific areas to ask follow-up questions, or choose from one of ChatGPT's suggested prompts to go deeper into your analysis. For example, you can ask ChatGPT to combine spreadsheets of monthly expenses and create a pivot table categorized by expense type.","resolvedUrl":"https:\/\/openai.com\/index\/improvements-to-data-analysis-in-chatgpt\/","title":"Improvements to data analysis in ChatGPT","sourceDomain":"openai.com"}},"socialMetadata":{"reactionTypeCounts":[{"type":"LIKE","count":1}],"entityUrn":"urn:li:fs_salesSocialMetadata:urn:li:activity:7197256909576417281","threadUrn":"urn:li:activity:7197256909576417281","reactionsCount":1,"commentsCount":0,"canComment":true},"activityUrl":"https:\/\/www.linkedin.com\/feed\/sales-navigator\/urn:li:share:7197256909081493505","message":{"attributes":[{"start":81,"length":5,"value":{"com.linkedin.common.HashtagAttributedEntity":{"hashtag":"urn:li:hashtag:llms"}}},{"start":87,"length":7,"value":{"com.linkedin.common.HashtagAttributedEntity":{"hashtag":"urn:li:hashtag:openai"}}}],"text":"chatGPT\u65b0\u589e\u4ea4\u4e92\u5f0f\u5716\u8868\u529f\u80fd\n\u652f\u63f4\u5f9eGoogle Drive\u6216 Microsoft OneDrive\u532f\u5165\u6587\u4ef6\u5f8c\uff0c\u8655\u7406\u8207\u5206\u6790\u6587\u4ef6\uff0c\u4e26\u4e14\u7522\u751f\u8996\u89ba\u5316\u5716\u7247\u5831\u8868\u3002\n\n\n#LLMs\n#OpenAI"},"entityUrn":"urn:li:share:7197256909081493505"}}},{"createdAt":1715604300000,"insightId":"aee92125-97fe-4979-ad8f-f41fb589b0ca","activityUnion":{"postActivity":{"contentSummaryUnion":{"ingestedContentSummary":{"image":{"attributes":[{"originalImageUrl":"https:\/\/media.licdn.com\/dms\/image\/sync\/D5627AQG_n2JI0wAeXQ\/articleshare-shrink_800\/0\/1716816930698?e=1717977600&v=beta&t=Nhwn-utdrwa-yn-GqvnmXPx8sLU2SRpR0IvVTRFA1rA","sourceType":"URL","imageUrl":"https:\/\/media.licdn.com\/dms\/image\/sync\/D5627AQG_n2JI0wAeXQ\/articleshare-shrink_800\/0\/1716816930698?e=1717977600&v=beta&t=Nhwn-utdrwa-yn-GqvnmXPx8sLU2SRpR0IvVTRFA1rA"}]},"description":"J41LBR34K PR0MPT5. Contribute to elder-plinius\/L1B3RT45 development by creating an account on GitHub.","resolvedUrl":"https:\/\/github.com\/elder-plinius\/L1B3RT45\/tree\/main","title":"GitHub - elder-plinius\/L1B3RT45: J41LBR34K PR0MPT5"}},"socialMetadata":{"reactionTypeCounts":[{"type":"LIKE","count":5}],"entityUrn":"urn:li:fs_salesSocialMetadata:urn:li:activity:7195766155431620609","threadUrn":"urn:li:activity:7195766155431620609","reactionsCount":5,"commentsCount":0,"canComment":true},"activityUrl":"https:\/\/www.linkedin.com\/feed\/sales-navigator\/urn:li:share:7195766154848608257","message":{"attributes":[{"start":64,"length":5,"value":{"com.linkedin.common.HashtagAttributedEntity":{"hashtag":"urn:li:hashtag:llms"}}},{"start":70,"length":9,"value":{"com.linkedin.common.HashtagAttributedEntity":{"hashtag":"urn:li:hashtag:text2img"}}}],"text":"\u4e3b\u6d41\u6a21\u578b(\u542bOpenAI\u3001Midjourney\u3001LLaMA 3\u3001DeepSeek\u3001Command R+\n\u7b49)\u4e4b\u8d8a\u7344\u63d0\u793a\u8a5e\u6536\u96c6\n\n#LLMs\n#Text2Img"},"entityUrn":"urn:li:share:7195766154848608257"}}},{"createdAt":1715588400000,"insightId":"74802eab-4fef-43b0-b532-512c7438a00b","activityUnion":{"postActivity":{"socialMetadata":{"reactionTypeCounts":[{"type":"LIKE","count":3}],"entityUrn":"urn:li:fs_salesSocialMetadata:urn:li:activity:7195699294379634689","threadUrn":"urn:li:activity:7195699294379634689","reactionsCount":3,"commentsCount":0,"canComment":true},"activityUrl":"https:\/\/www.linkedin.com\/feed\/sales-navigator\/urn:li:share:7195699293750472705","message":{"attributes":[{"start":98,"length":24,"value":{"com.linkedin.common.HyperlinkAttributedEntity":{"url":"https:\/\/lnkd.in\/dtiM7iwb"}}},{"start":159,"length":24,"value":{"com.linkedin.common.HyperlinkAttributedEntity":{"url":"https:\/\/lnkd.in\/dh3T6Acu"}}},{"start":225,"length":5,"value":{"com.linkedin.common.HashtagAttributedEntity":{"hashtag":"urn:li:hashtag:llms"}}},{"start":231,"length":11,"value":{"com.linkedin.common.HashtagAttributedEntity":{"hashtag":"urn:li:hashtag:gradientai"}}}],"text":"Gradientai\u4e4b\u524d\u6709\u5c07LLmam 3 70b\u4f7f\u7528\u6f38\u9032\u5f0f\u8a13\u7df4\u589e\u52a0Content length\u81f31048k\n\n\u6700\u8fd1\u53c8\u516c\u4f48\u4e86\nLlama-3-8B-Instruct-Gradient-1048k\nhttps:\/\/lnkd.in\/dtiM7iwb\n\nLlama-3-8B-Instruct-Gradient-4194k\nhttps:\/\/lnkd.in\/dh3T6Acu\n\n\u8a71\u8aaaLlama 3 8B 4194k\u7684\u5927\u6d77\u6488\u91dd\u6e2c\u8a66\u771f\u662f\u6158\uff0c\u4e0d\u904e\u4e5f\u4e0d\u610f\u5916\u5c31\u662f\u4e86\n\n#LLMs\n#Gradientai"},"entityUrn":"urn:li:share:7195699293750472705"}}},{"createdAt":1715520660000,"insightId":"843d9117-2991-487a-a438-ec98a0869235","activityUnion":{"postActivity":{"contentSummaryUnion":{"ingestedContentSummary":{"image":{"attributes":[{"originalImageUrl":"https:\/\/media.licdn.com\/dms\/image\/sync\/D5627AQEqUqRF-IH1hg\/articleshare-shrink_800\/0\/1715520298914?e=1717977600&v=beta&t=5jd-LfX26MaAKsjoRqlR9YS__xgOQsMYKjdpIMy3tGI","sourceType":"URL","imageUrl":"https:\/\/media.licdn.com\/dms\/image\/sync\/D5627AQEqUqRF-IH1hg\/articleshare-shrink_800\/0\/1715520298914?e=1717977600&v=beta&t=5jd-LfX26MaAKsjoRqlR9YS__xgOQsMYKjdpIMy3tGI"}]},"description":"Optimizing the deployment of Large language models (LLMs) is expensive today since it requires experimentally running an application workload against an LLM implementation while exploring large configuration space formed by system knobs such as...","resolvedUrl":"https:\/\/arxiv.org\/abs\/2405.05465v1","title":"Vidur: A Large-Scale Simulation Framework For LLM Inference"}},"socialMetadata":{"reactionTypeCounts":[{"type":"LIKE","count":4}],"entityUrn":"urn:li:fs_salesSocialMetadata:urn:li:activity:7195415200878010370","threadUrn":"urn:li:activity:7195415200878010370","reactionsCount":4,"commentsCount":0,"canComment":true},"activityUrl":"https:\/\/www.linkedin.com\/feed\/sales-navigator\/urn:li:share:7195415200425021440","message":{"attributes":[{"start":157,"length":24,"value":{"com.linkedin.common.HyperlinkAttributedEntity":{"url":"https:\/\/lnkd.in\/gA6VzEA9"}}},{"start":184,"length":5,"value":{"com.linkedin.common.HashtagAttributedEntity":{"hashtag":"urn:li:hashtag:llms"}}},{"start":190,"length":10,"value":{"com.linkedin.common.HashtagAttributedEntity":{"hashtag":"urn:li:hashtag:microsoft"}}}],"text":"Vidur\nLLM\u7684\u90e8\u7f72\u6210\u672c\u9ad8\u6602\uff0c\u6d89\u53ca\u5230\u4e00\u7cfb\u5217\u7684\u7dad\u904b\u53ca\u8a2d\u5b9a\u7b56\u7565\u3002\n\u5fae\u8edf\u958b\u6e90\u7684Vidur\u4f7f\u7528\u5be6\u9a57\u6027\u80fd\u5206\u6790\u548c\u9810\u6e2c\u6a21\u578b\u4f86\u6a21\u64ecLLM\u7684\u904b\u7b97\u6548\u80fd\uff0c\u80fd\u5728\u591a\u7a2e\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u4e0a\u4f30\u8a08\u7684\u63a8\u7406\u5ef6\u9072\u8207\u5be6\u969b\u8aa4\u5dee\u4f4e\u65bc9%\u3002\n\u5b83\u63d0\u4f9b\u7684\u81ea\u52d5\u5316\u8a2d\u5b9a\u641c\u5c0b\u5de5\u5177(Vidur-Search)\uff0c\u627e\u5230\u6eff\u8db3\u61c9\u7528\u7a0b\u5f0f\u6548\u80fd\u9650\u5236\u7684\u6700\u5177\u6210\u672c\u6548\u76ca\u7684\u90e8\u7f72\u914d\u7f6e\u3002\n\n\u7a0b\u5f0f\u78bc\uff1a\nhttps:\/\/lnkd.in\/gA6VzEA9\n\n\n#LLMs\n#Microsoft"},"entityUrn":"urn:li:share:7195415200425021440"}}},{"createdAt":1715438040000,"insightId":"0cf71aba-8b9c-4939-aca3-2186cc360199","activityUnion":{"postActivity":{"socialMetadata":{"reactionTypeCounts":[{"type":"LIKE","count":4},{"type":"ENTERTAINMENT","count":1}],"entityUrn":"urn:li:fs_salesSocialMetadata:urn:li:activity:7195068882955505664","threadUrn":"urn:li:activity:7195068882955505664","reactionsCount":5,"commentsCount":0,"canComment":true},"activityUrl":"https:\/\/www.linkedin.com\/feed\/sales-navigator\/urn:li:share:7195068882544472064","message":{"attributes":[{"start":19,"length":24,"value":{"com.linkedin.common.HyperlinkAttributedEntity":{"url":"https:\/\/lnkd.in\/gKn7-et4"}}},{"start":45,"length":5,"value":{"com.linkedin.common.HashtagAttributedEntity":{"hashtag":"urn:li:hashtag:llms"}}},{"start":51,"length":7,"value":{"com.linkedin.common.HashtagAttributedEntity":{"hashtag":"urn:li:hashtag:openai"}}}],"text":"OpenAI\u505a\u641c\u7d22\u5f15\u64ce\u770b\u4f86\u53ea\u662f\u8a98\u990c\n\nhttps:\/\/lnkd.in\/gKn7-et4\n\n#LLMs\n#OpenAI"},"entityUrn":"urn:li:share:7195068882544472064"}}},{"createdAt":1715429460000,"insightId":"5c5155f6-e6a1-4bc5-8d13-daf5decce721","activityUnion":{"postActivity":{"socialMetadata":{"reactionTypeCounts":[{"type":"LIKE","count":2}],"entityUrn":"urn:li:fs_salesSocialMetadata:urn:li:activity:7195032674816704512","threadUrn":"urn:li:activity:7195032674816704512","reactionsCount":2,"commentsCount":1,"canComment":true},"activityUrl":"https:\/\/www.linkedin.com\/feed\/sales-navigator\/urn:li:share:7195032674279862272","message":{"attributes":[{"start":86,"length":24,"value":{"com.linkedin.common.HyperlinkAttributedEntity":{"url":"https:\/\/lnkd.in\/gXdMAv6u"}}},{"start":112,"length":5,"value":{"com.linkedin.common.HashtagAttributedEntity":{"hashtag":"urn:li:hashtag:llms"}}},{"start":118,"length":7,"value":{"com.linkedin.common.HashtagAttributedEntity":{"hashtag":"urn:li:hashtag:llama3"}}}],"text":"Llama 3 \u7684 Tokenizer \u914d\u7f6e\u66f4\u52d5\uff1a\neos_token \u5f9e <|end_of_text|> \u6539\u70ba <|eot_id|>\n\n\n\u8cc7\u6599\u4f86\u6e90(\u9700\u8981\u6709\u6388\u6b0a\u624d\u80fd\u770b)\uff1a\nhttps:\/\/lnkd.in\/gXdMAv6u\n\n#LLMs\n#Llama3"},"entityUrn":"urn:li:share:7195032674279862272"}}},{"createdAt":1715428680000,"insightId":"0d3d2afe-6ab8-4aee-ba56-30aff51164b1","activityUnion":{"postActivity":{"contentSummaryUnion":{"ingestedContentSummary":{"image":{"attributes":[{"originalImageUrl":"https:\/\/media.licdn.com\/dms\/image\/sync\/D5627AQEARpN0c7_RiA\/articleshare-shrink_800\/0\/1715428531800?e=1717977600&v=beta&t=rsFHXkC_TOBX8OFLP-tOQn4uPuRoLYZPfG9JlcKV-J4","sourceType":"URL","imageUrl":"https:\/\/media.licdn.com\/dms\/image\/sync\/D5627AQEARpN0c7_RiA\/articleshare-shrink_800\/0\/1715428531800?e=1717977600&v=beta&t=rsFHXkC_TOBX8OFLP-tOQn4uPuRoLYZPfG9JlcKV-J4"}]},"description":"We\u2019re on a journey to advance and democratize artificial intelligence through open source and open science.","resolvedUrl":"https:\/\/huggingface.co\/omi-health\/sum-small","title":"omi-health\/sum-small \u00b7 Hugging Face"}},"socialMetadata":{"reactionTypeCounts":[{"type":"LIKE","count":9}],"entityUrn":"urn:li:fs_salesSocialMetadata:urn:li:activity:7195029622495875072","threadUrn":"urn:li:activity:7195029622495875072","reactionsCount":9,"commentsCount":0,"canComment":true},"activityUrl":"https:\/\/www.linkedin.com\/feed\/sales-navigator\/urn:li:share:7195029621824794624","message":{"attributes":[{"start":71,"length":5,"value":{"com.linkedin.common.HashtagAttributedEntity":{"hashtag":"urn:li:hashtag:llms"}}}],"text":"sum-small\n\u4ee5Phi 3\u70ba\u57fa\u5e95\u8a13\u7df4\u7684\u91ab\u5b78\u9818\u57df\u7279\u5316\u6a21\u578b\uff0c\u6027\u80fd\u6e2c\u8a66\u7d50\u679c\u8d85\u904eGPT4 Turbo\nMIT License\u6388\u6b0a\uff0c\u53ef\u7528\u65bc\u5546\u696d\n\n#LLMs"},"entityUrn":"urn:li:share:7195029621824794624"}}},{"createdAt":1715422560000,"insightId":"aaf50ddf-dca7-460a-99fd-7f46f700eccd","activityUnion":{"postActivity":{"contentSummaryUnion":{"ingestedContentSummary":{"image":{"attributes":[{"originalImageUrl":"https:\/\/media.licdn.com\/dms\/image\/sync\/D4E27AQEvynTpGtcuvw\/articleshare-shrink_800\/0\/1715299254752?e=1717977600&v=beta&t=kErKnv9-rj9DwARD2Bmywe2BOr21IQhkwRnRnJjuERM","sourceType":"URL","imageUrl":"https:\/\/media.licdn.com\/dms\/image\/sync\/D4E27AQEvynTpGtcuvw\/articleshare-shrink_800\/0\/1715299254752?e=1717977600&v=beta&t=kErKnv9-rj9DwARD2Bmywe2BOr21IQhkwRnRnJjuERM"}]},"description":"Motivation","resolvedUrl":"https:\/\/medium.com\/@akshgarg_36829\/gemma-10m-technical-overview-900adc4fbeeb","title":"Gemma-10M Technical Overview"}},"socialMetadata":{"reactionTypeCounts":[{"type":"LIKE","count":6}],"entityUrn":"urn:li:fs_salesSocialMetadata:urn:li:activity:7195003737436794880","threadUrn":"urn:li:activity:7195003737436794880","reactionsCount":6,"commentsCount":2,"canComment":true},"activityUrl":"https:\/\/www.linkedin.com\/feed\/sales-navigator\/urn:li:share:7195003736740515840","message":{"attributes":[{"start":74,"length":24,"value":{"com.linkedin.common.HyperlinkAttributedEntity":{"url":"https:\/\/lnkd.in\/gFst5p_v"}}},{"start":106,"length":24,"value":{"com.linkedin.common.HyperlinkAttributedEntity":{"url":"https:\/\/lnkd.in\/gy6mN7PK"}}},{"start":132,"length":5,"value":{"com.linkedin.common.HashtagAttributedEntity":{"hashtag":"urn:li:hashtag:llms"}}},{"start":138,"length":6,"value":{"com.linkedin.common.HashtagAttributedEntity":{"hashtag":"urn:li:hashtag:gemma"}}}],"text":"Gemma 2B \u7684Content Length 10M\u7248\u672c\uff0c\u6210\u529f\u572832GB ram\u4e2d\u57f7\u884c\uff0c\u9019\u5c0d\u672a\u4f86\u7814\u7a76\u5982\u4f55\u96e2\u7dda\u904b\u8dd1\u65bc\u624b\u6a5f\u4e0a\u6709\u6240\u5e6b\u52a9\u3002\n\n\u6a21\u578b\u4e0b\u8f09\nhttps:\/\/lnkd.in\/gFst5p_v\n\n\u63a8\u7406\u7a0b\u5f0f\u78bc\nhttps:\/\/lnkd.in\/gy6mN7PK\n\n#LLMs\n#Gemma"},"entityUrn":"urn:li:share:7195003736740515840"}}},{"createdAt":1715393100000,"insightId":"fcd77929-5302-47ca-b0cb-cf1aa6ec5361","activityUnion":{"postActivity":{"socialMetadata":{"reactionTypeCounts":[{"type":"LIKE","count":3}],"entityUrn":"urn:li:fs_salesSocialMetadata:urn:li:activity:7194880373497946112","threadUrn":"urn:li:activity:7194880373497946112","reactionsCount":3,"commentsCount":0,"canComment":true},"activityUrl":"https:\/\/www.linkedin.com\/feed\/sales-navigator\/urn:li:share:7194880372759834625","message":{"attributes":[{"start":80,"length":24,"value":{"com.linkedin.common.HyperlinkAttributedEntity":{"url":"https:\/\/lnkd.in\/gsQgmpWK"}}},{"start":140,"length":24,"value":{"com.linkedin.common.HyperlinkAttributedEntity":{"url":"https:\/\/lnkd.in\/gmgzFizB"}}},{"start":167,"length":5,"value":{"com.linkedin.common.HashtagAttributedEntity":{"hashtag":"urn:li:hashtag:llms"}}},{"start":173,"length":7,"value":{"com.linkedin.common.HashtagAttributedEntity":{"hashtag":"urn:li:hashtag:openai"}}}],"text":"\u5b98\u65b9\u516c\u544a\u51fa\u4f86\u4e86\uff0c\u8209\u8fa6\u6642\u9593\u70ba\n10AM PT Monday, May 13\n\u4e5f\u5c31\u662f\n\u53f0\u5317\u6642\u9593 5\u670814\u65e5(\u4e8c) AM 01:00\n\u53ef\u5728\u5b98\u7db2\u89c0\u770b\u76f4\u64ad\n\n\u8cc7\u6599\u4f86\u6e90\uff1a\nhttps:\/\/lnkd.in\/gsQgmpWK\n\n\nSam Altman\u88dc\u5145\u8aaa\u5c07\u4e0d\u6703\u767c\u5e03GPT-5\uff0c\u4e5f\u4e0d\u6703\u6709\u641c\u7d22\u5f15\u64ce\nhttps:\/\/lnkd.in\/gmgzFizB\n\n\n#LLMs\n#OpenAI"},"rootActivity":{"activityUrl":"https:\/\/www.linkedin.com\/feed\/sales-navigator\/urn:li:share:7193602854287282176","message":{"attributes":[{"start":27,"length":24,"value":{"com.linkedin.common.HyperlinkAttributedEntity":{"url":"https:\/\/lnkd.in\/g_MvKmSd"}}},{"start":53,"length":5,"value":{"com.linkedin.common.HashtagAttributedEntity":{"hashtag":"urn:li:hashtag:llms"}}},{"start":59,"length":7,"value":{"com.linkedin.common.HashtagAttributedEntity":{"hashtag":"urn:li:hashtag:openai"}}}],"text":"OpenAI \u7684 5\/9\u767c\u5e03\u6703\u53ef\u80fd\u6703\u5ef6\u671f\n\u6d88\u606f\u4f86\u6e90\uff1a\nhttps:\/\/lnkd.in\/g_MvKmSd\n\n#LLMs\n#OpenAI"},"entityUrn":"urn:li:share:7193602854287282176"},"entityUrn":"urn:li:share:7194880372759834625"}}},{"createdAt":1715263440000,"insightId":"50544ee6-34f8-496d-b781-46c7c60ed805","activityUnion":{"postActivity":{"contentSummaryUnion":{"ingestedContentSummary":{"image":{"attributes":[{"originalImageUrl":"https:\/\/media.licdn.com\/dms\/image\/sync\/D5627AQGowaADfws_2Q\/articleshare-shrink_800\/0\/1715261663140?e=1717977600&v=beta&t=EttPNAwnxIvxv_gXO1LP0F1v2AA_qas2-T8nlGvjy2A","sourceType":"URL","imageUrl":"https:\/\/media.licdn.com\/dms\/image\/sync\/D5627AQGowaADfws_2Q\/articleshare-shrink_800\/0\/1715261663140?e=1717977600&v=beta&t=EttPNAwnxIvxv_gXO1LP0F1v2AA_qas2-T8nlGvjy2A"}]},"description":"We introduce a decoder-decoder architecture, YOCO, for large language models, which only caches key-value pairs once. It consists of two components, i.e., a cross-decoder stacked upon a self-decoder. The self-decoder efficiently encodes global...","resolvedUrl":"https:\/\/arxiv.org\/abs\/2405.05254","title":"You Only Cache Once: Decoder-Decoder Architectures for Language Models"}},"socialMetadata":{"reactionTypeCounts":[{"type":"LIKE","count":16},{"type":"EMPATHY","count":1}],"entityUrn":"urn:li:fs_salesSocialMetadata:urn:li:activity:7194336312189939712","threadUrn":"urn:li:activity:7194336312189939712","reactionsCount":17,"commentsCount":0,"canComment":true},"activityUrl":"https:\/\/www.linkedin.com\/feed\/sales-navigator\/urn:li:share:7194336311573372929","message":{"attributes":[{"start":320,"length":24,"value":{"com.linkedin.common.HyperlinkAttributedEntity":{"url":"https:\/\/lnkd.in\/gFcXa-Dm"}}},{"start":347,"length":5,"value":{"com.linkedin.common.HashtagAttributedEntity":{"hashtag":"urn:li:hashtag:llms"}}},{"start":353,"length":10,"value":{"com.linkedin.common.HashtagAttributedEntity":{"hashtag":"urn:li:hashtag:microsoft"}}}],"text":"YOCO (You Only Cache Once)\nYOCO\u986f\u8457\u964d\u4f4e\u4e86GPU\u8a18\u61b6\u9ad4\u6d88\u8017\uff0c\u7531\u5169\u500b\u90e8\u5206\u7d44\u6210\n1. \u4ea4\u53c9\u89e3\u78bc\u5668(cross decoder\u00a0)\n2. \u81ea\u89e3\u78bc\u5668(Self-decoder)\n\n\u81ea\u89e3\u78bc\u5668\u7de8\u78bc\u5168\u57df\u5feb\u53d6\u9375\u503c(KV)\uff0c\u4e26\u7531\u4ea4\u53c9\u89e3\u78bc\u5668\u900f\u904e\u4ea4\u53c9\u6ce8\u610f\u529b\u6a5f\u5236(cross-attention mechanism)\u91cd\u8907\u4f7f\u7528\u3002\n\n\u5be6\u9a57\u7d50\u679c\u986f\u793a:\n- \u5728\u4e0d\u540c\u8a2d\u5b9a\u4e0b\u653e\u5927\u6a21\u578b\u5c3a\u5bf8\u53ca\u8a13\u7df4Tokens\u6578\u91cf\u6642\uff0cYOCO\u76f8\u8f03\u65bcTransformer\u67b6\u69cb\u8868\u73fe\u66f4\u512a\u79c0\u3002\n- \u64f4\u5c55\u5230 1M Contet Length\uff0c\u5177\u6709\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u5927\u6d77\u6488\u91dd\u8868\u73fe\n- \u986f\u8457\u964d\u4f4e\u4e86GPU\u8a18\u61b6\u9ad4\u6d88\u8017\n  65B \u6a21\u578b\u4e2d\uff0cKV \u5feb\u53d6\u7684\u8a18\u61b6\u9ad4\u53ef\u6e1b\u5c11\u7d04 80 \u500d\u3002\n\n\n\u7a0b\u5f0f\u78bc\uff1a\nhttps:\/\/lnkd.in\/gFcXa-Dm\n\n\n#LLMs\n#Microsoft"},"entityUrn":"urn:li:share:7194336311573372929"}}}]}